{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"data/\"\n",
    "data=pd.read_csv(path+\"input_train.csv\", index_col=\"ID\")\n",
    "X_ACC=data.filter(regex= \"ACC*\")\n",
    "X_EEG=data.filter(regex= \"EEG*\")\n",
    "labels=pd.read_csv(path+\"challenge_output_data_training_file_sleep_stages_classification.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def create_input_features(X, n_channels, return_y=False):\n",
    "    \"\"\"\n",
    "    takes values from one signal on n_channels and:\n",
    "    - Divides it on train, validation and test\n",
    "    - scales them\n",
    "    - and formats them for use on the NN\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels[\"TARGET\"].values, test_size=0.10, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.10, random_state=42)\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    if return_y:\n",
    "        return (\n",
    "            X_train.reshape(X_train.shape[0],n_channels,X_train.shape[1]/n_channels), y_train,\n",
    "            X_val.reshape(X_val.shape[0],n_channels,X_val.shape[1]/n_channels), y_val,\n",
    "            X_test.reshape(X_test.shape[0],n_channels,X_test.shape[1]/n_channels), y_test     \n",
    "        )  \n",
    "    else:\n",
    "        return (\n",
    "            X_train.reshape(X_train.shape[0],n_channels,X_train.shape[1]/n_channels), \n",
    "            X_val.reshape(X_val.shape[0],n_channels,X_val.shape[1]/n_channels),\n",
    "            X_test.reshape(X_test.shape[0],n_channels,X_test.shape[1]/n_channels)        \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ACC_train, X_ACC_val, X_ACC_test=create_input_features(X_ACC, 3)\n",
    "X_EEG_train, y_train, X_EEG_val, y_val, X_EEG_test, y_test=create_input_features(X_EEG,1,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_cnn(input_var_EEG=None, input_var_ACC=None):\n",
    "    #Two initialisation layers\n",
    "    #One for EEG\n",
    "    l_in1 = lasagne.layers.InputLayer(shape=(None,X_EEG_train.shape[1], X_EEG_train.shape[2]),\n",
    "                                        input_var=input_var_EEG)\n",
    "    #One for ACC\n",
    "    l_in2 = lasagne.layers.InputLayer(shape=(None,X_ACC_train.shape[1], X_ACC_train.shape[2]),\n",
    "                                        input_var=input_var_ACC)\n",
    "    #Two convolutional layers to treat each signal separatedly\n",
    "    l_conv1_1 = lasagne.layers.Conv1DLayer(\n",
    "        l_in1, num_filters=8, filter_size=50,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    l_conv2_1 = lasagne.layers.Conv1DLayer(\n",
    "        l_in2, num_filters=8, filter_size=5,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    #And again another 2 layers\n",
    "    l_conv1_2 = lasagne.layers.Conv1DLayer(\n",
    "        l_conv1_1, num_filters=8, filter_size=50,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())\n",
    "    l_conv2_2 = lasagne.layers.Conv1DLayer(\n",
    "        l_conv2_1, num_filters=8, filter_size=5,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify,\n",
    "        W=lasagne.init.GlorotUniform())   \n",
    "    #And two pooling layers\n",
    "    l_pool1 = lasagne.layers.MaxPool1DLayer(l_conv1_2, pool_size=10)\n",
    "    l_pool2 = lasagne.layers.MaxPool1DLayer(l_conv2_2, pool_size=1)\n",
    "\n",
    "    #Two dense layers for each signal\n",
    "    l_dense_1_1 = lasagne.layers.DenseLayer(\n",
    "        lasagne.layers.dropout(l_pool1, p=.25),\n",
    "        num_units=500,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    l_dense_2_1 = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(l_pool2, p=.25),\n",
    "        num_units=500,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    #use neurons from both signals to predict\n",
    "    #concatenate neurons\n",
    "    l_concat = lasagne.layers.ConcatLayer([l_dense_1_1, l_dense_2_1], axis=1)\n",
    "    #dense layer with all neurons\n",
    "    l_dense_2 = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(l_concat, p=.25),\n",
    "        num_units=1000,\n",
    "        nonlinearity=lasagne.nonlinearities.rectify)\n",
    "    #Output layer\n",
    "    l_output = lasagne.layers.DenseLayer(\n",
    "        lasagne.layers.dropout(l_dense_2, p=.25),\n",
    "        num_units=5,\n",
    "        nonlinearity=lasagne.nonlinearities.softmax)\n",
    "\n",
    "    return l_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var_EEG = T.tensor3('inputs')\n",
    "input_var_ACC = T.tensor3('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "# Create neural network model\n",
    "network = build_cnn(input_var_EEG, input_var_ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = lasagne.layers.get_output(network)\n",
    "loss = lasagne.objectives.multiclass_hinge_loss(prediction, target_var)\n",
    "loss = loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "        loss, params, learning_rate=0.0005, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "test_loss = lasagne.objectives.multiclass_hinge_loss(test_prediction,\n",
    "                                                        target_var)\n",
    "test_loss = test_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var),\n",
    "                  dtype=theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tempfile import mkstemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tempfile.mkstemp(dir=\"../tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.environ[\"THEANO_FLAGS\"] = \"blas.ldflags=-llapack -lblas -lgfortran\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_fn =theano.function([input_var_EEG, input_var_ACC, target_var], loss, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_fn = theano.function([input_var_EEG, input_var_ACC,target_var], [test_loss, test_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(input_1, input_2, targets, batchsize, shuffle=False):\n",
    "    assert len(input_1) == len(targets)\n",
    "    assert len(input_2) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(input_1))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(input_1) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield input_1[excerpt], input_2[excerpt], targets[excerpt]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train batches10\n",
      "epoch 0, train batches20\n",
      "epoch 0, train batches30\n",
      "epoch 0, train batches40\n",
      "epoch 0, train batches50\n",
      "epoch 0, train batches60\n",
      "epoch 0, train batches70\n",
      "epoch 0, train batches80\n",
      "epoch 0, train batches90\n",
      "epoch 0, train batches100\n",
      "epoch 0, train batches110\n",
      "epoch 0, train batches120\n",
      "epoch 0, train batches130\n",
      "epoch 0, train batches140\n",
      "epoch 0, train batches150\n",
      "epoch 0, train batches160\n",
      "epoch 0, train batches170\n",
      "epoch 0, train batches180\n",
      "epoch 0, train batches190\n",
      "epoch 0, train batches200\n",
      "epoch 0, train batches210\n",
      "epoch 0, train batches220\n",
      "epoch 0, train batches230\n",
      "epoch 0, train batches240\n",
      "epoch 0, train batches250\n",
      "epoch 0, train batches260\n",
      "epoch 0, train batches270\n",
      "epoch 0, train batches280\n",
      "epoch 0, train batches290\n",
      "epoch 0, train batches300\n",
      "epoch 0, train batches310\n",
      "epoch 0, train batches320\n",
      "epoch 0, train batches330\n",
      "epoch 0, train batches340\n",
      "epoch 0, train batches350\n",
      "epoch 0, train batches360\n",
      "epoch 0, train batches370\n",
      "epoch 0, train batches380\n",
      "epoch 0, train batches390\n",
      "epoch 0, val batches10\n",
      "epoch 0, val batches20\n",
      "epoch 0, val batches30\n",
      "epoch 0, val batches40\n",
      "Epoch 1 of 20 took 1068.192s\n",
      "  training loss:\t\t1.010445\n",
      "  validation loss:\t\t0.971543\n",
      "  validation accuracy:\t\t47.93 %\n",
      "epoch 1, train batches10\n",
      "epoch 1, train batches20\n",
      "epoch 1, train batches30\n",
      "epoch 1, train batches40\n",
      "epoch 1, train batches50\n",
      "epoch 1, train batches60\n",
      "epoch 1, train batches70\n",
      "epoch 1, train batches80\n",
      "epoch 1, train batches90\n",
      "epoch 1, train batches100\n",
      "epoch 1, train batches110\n",
      "epoch 1, train batches120\n",
      "epoch 1, train batches130\n",
      "epoch 1, train batches140\n",
      "epoch 1, train batches150\n",
      "epoch 1, train batches160\n",
      "epoch 1, train batches170\n",
      "epoch 1, train batches180\n",
      "epoch 1, train batches190\n",
      "epoch 1, train batches200\n",
      "epoch 1, train batches210\n",
      "epoch 1, train batches220\n",
      "epoch 1, train batches230\n",
      "epoch 1, train batches240\n",
      "epoch 1, train batches250\n",
      "epoch 1, train batches260\n",
      "epoch 1, train batches270\n",
      "epoch 1, train batches280\n",
      "epoch 1, train batches290\n",
      "epoch 1, train batches300\n",
      "epoch 1, train batches310\n",
      "epoch 1, train batches320\n",
      "epoch 1, train batches330"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    train_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_EEG_train, X_ACC_train, y_train, 64, shuffle=True):\n",
    "        input_1, input_2, targets = batch\n",
    "        targets = targets.astype(np.int32)\n",
    "        train_err += train_fn(input_1, input_2, targets)\n",
    "        train_batches += 1\n",
    "        if train_batches % 10 == 0:\n",
    "            print \"epoch {}, train batches{}\".format(epoch,train_batches) \n",
    "\n",
    "    # And a full pass over the validation data:\n",
    "    val_err = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for batch in iterate_minibatches(X_EEG_val, X_ACC_val, y_val, 64, shuffle=False):\n",
    "        input_1, input_2, targets = batch\n",
    "        targets = targets.astype(np.int32)\n",
    "        err, acc = val_fn(input_1, input_2, targets)\n",
    "        val_err += err\n",
    "        val_acc += acc\n",
    "        val_batches += 1\n",
    "        if val_batches % 10 == 0:\n",
    "            print \"epoch {}, val batches{}\".format(epoch,val_batches) \n",
    "\n",
    "    # Then we print the results for this epoch:\n",
    "    print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "        epoch + 1, num_epochs, time.time() - start_time))\n",
    "    print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "    print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "    print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "        val_acc / val_batches * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'TensorVariable' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-36c55cb3e96f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpredicted_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_prediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"test loss:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'TensorVariable' object is not callable"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predicted_values = test_prediction.flatten()\n",
    "print \"test loss:\", test_loss(predicted_values,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reshape{1}.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
