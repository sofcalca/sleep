{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "theano/tensor/signal/downsample.py:5: UserWarning: downsample module has been moved to the pool module.\n",
      "  warnings.warn(\"downsample module has been moved to the pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path=\"data/\"\n",
    "data=pd.read_csv(path+\"input_train.csv\", index_col=\"ID\")\n",
    "X_ACC=data.filter(regex= \"ACC*\")\n",
    "X_EEG=data.filter(regex= \"EEG*\")\n",
    "labels=pd.read_csv(path+\"challenge_output_data_training_file_sleep_stages_classification.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_test=pd.read_csv(path+\"input_test.csv\", index_col=\"ID\")\n",
    "X_ACC_TEST=data_test.filter(regex= \"ACC*\")\n",
    "X_EEG_TEST=data_test.filter(regex= \"EEG*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "def train_val_test_split(X):    \n",
    "    X_train, X_test = train_test_split(X, test_size=0.10, random_state=42)\n",
    "    X_train, X_val = train_test_split(X_train, test_size=0.10, random_state=42)\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ACC_train, X_ACC_val, X_ACC_test=train_val_test_split(X_ACC)\n",
    "X_EEG_train, X_EEG_val, X_EEG_test=train_val_test_split(X_EEG)\n",
    "y_train, y_val, y_test=train_val_test_split(labels[\"TARGET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30458, 450), (31129, 450))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ACC_TEST.shape, X_ACC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30458, 450)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ACC_TEST.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_ACC_train = StandardScaler().fit(X_ACC_train)\n",
    "scaler_EEG_train= StandardScaler().fit(X_EEG_train)\n",
    "scaler_ACC_predict = StandardScaler().fit(X_ACC)\n",
    "scaler_EEG_predict= StandardScaler().fit(X_EEG)\n",
    "\n",
    "X_ACC_train=scaler_ACC_train.transform(X_ACC_train)\n",
    "X_ACC_val=scaler_ACC_train.transform(X_ACC_val)\n",
    "X_ACC_test=scaler_ACC_train.transform(X_ACC_test)\n",
    "\n",
    "X_EEG_train=scaler_EEG_train.transform(X_EEG_train)\n",
    "X_EEG_val=scaler_EEG_train.transform(X_EEG_val)\n",
    "X_EEG_test=scaler_EEG_train.transform(X_EEG_test)\n",
    "\n",
    "X_ACC=scaler_ACC_predict.transform(X_ACC)\n",
    "X_EEG=scaler_EEG_predict.transform(X_EEG)\n",
    "\n",
    "X_ACC_TEST=scaler_ACC_predict.transform(X_ACC_TEST)\n",
    "X_EEG_TEST=scaler_EEG_predict.transform(X_EEG_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reshape(X, n_channels):\n",
    "    return X.reshape(X.shape[0],n_channels,X.shape[1]/n_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_EEG_train=reshape(X_EEG_train,1)\n",
    "X_EEG_val=reshape(X_EEG_val,1)\n",
    "X_EEG_test=reshape(X_EEG_test,1)\n",
    "X_EEG=reshape(X_EEG,1)\n",
    "X_EEG_TEST=reshape(X_EEG_TEST,1)\n",
    "\n",
    "X_ACC_train=reshape(X_ACC_train,3)\n",
    "X_ACC_val=reshape(X_ACC_val,3)\n",
    "X_ACC_test=reshape(X_ACC_test,3)\n",
    "X_ACC=reshape(X_ACC,3)\n",
    "X_ACC_TEST=reshape(X_ACC_TEST,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare Theano variables for inputs and targets\n",
    "input_var_EEG = T.tensor3('inputs')\n",
    "input_var_ACC = T.tensor3('inputs')\n",
    "target_var = T.ivector('targets')\n",
    "# Create neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.base import BaseEstimator\n",
    "def iterate_minibatches(input_1, input_2, targets, batchsize, shuffle=False):\n",
    "    assert len(input_1) == len(targets)\n",
    "    assert len(input_2) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(input_1))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(input_1) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield input_1[excerpt], input_2[excerpt], targets[excerpt]\n",
    "\n",
    "\"\"\"Default variables\"\"\"\n",
    "num_epochs = 15\n",
    "EEG_shape_1=X_EEG_train.shape[1]\n",
    "EEG_shape_2=X_EEG_train.shape[2]\n",
    "ACC_shape_1=X_ACC_train.shape[1]\n",
    "ACC_shape_2=X_ACC_train.shape[2]\n",
    "n_filters_EEG_1=32\n",
    "#n_filters_EEG_2=\n",
    "n_filters_ACC_1=16\n",
    "#n_filters_ACC_2=\n",
    "filter_size_EEG_1=20\n",
    "#filter_size_EEG_2=\n",
    "filter_size_ACC_1=6\n",
    "#filter_size_ACC_2=\n",
    "pool_size_EEG=5\n",
    "pool_size_ACC=2\n",
    "drop_out=0.5\n",
    "init_learning_rate=0.01\n",
    "num_units_EEG=1000\n",
    "num_units_ACC=100\n",
    "num_units_pool=2000\n",
    "num_classes=5\n",
    "\n",
    "class CNN(BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_var_EEG, \n",
    "        input_var_ACC, \n",
    "        target_var,\n",
    "        num_epochs\n",
    "    ):\n",
    "        #setting variables\n",
    "        self.input_var_EEG=input_var_EEG\n",
    "        self.input_var_ACC=input_var_ACC\n",
    "        self.target_var=target_var    \n",
    "        self.num_epochs=num_epochs\n",
    "        \n",
    "        #The network\n",
    "        #Two initialisation layers\n",
    "        #One for EEG\n",
    "        self.l_in1 = lasagne.layers.InputLayer(\n",
    "            shape=(None,EEG_shape_1, EEG_shape_2),\n",
    "            input_var=self.input_var_EEG\n",
    "        )\n",
    "        #One for ACC\n",
    "        self.l_in2 = lasagne.layers.InputLayer(\n",
    "            shape=(None,ACC_shape_1, ACC_shape_2),\n",
    "            input_var=self.input_var_ACC\n",
    "        )\n",
    "        #Two convolutional layers to treat each signal separatedly\n",
    "        self.l_conv1_1 = lasagne.layers.Conv1DLayer(\n",
    "            self.l_in1, \n",
    "            num_filters=n_filters_EEG_1, \n",
    "            filter_size=filter_size_EEG_1,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotNormal()\n",
    "        )\n",
    "        self.l_conv2_1 = lasagne.layers.Conv1DLayer(\n",
    "            self.l_in2, \n",
    "            num_filters=n_filters_ACC_1, \n",
    "            filter_size=filter_size_ACC_1,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotNormal()\n",
    "        )\n",
    "        #And again another 2 layers\n",
    "        \"\"\"l_conv1_2 = lasagne.layers.Conv1DLayer(\n",
    "            l_conv1_1, num_filters=16, filter_size=50,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "        l_conv2_2 = lasagne.layers.Conv1DLayer(\n",
    "            l_conv2_1, num_filters=16, filter_size=5,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform())\n",
    "        \"\"\"\n",
    "        #And two pooling layers\n",
    "        \"\"\"l_pool1 = lasagne.layers.MaxPool1DLayer(l_conv1_2, pool_size=25)\n",
    "        l_pool2 = lasagne.layers.MaxPool1DLayer(l_conv2_2, pool_size=2)\n",
    "        \"\"\"\n",
    "        self.l_pool1 = lasagne.layers.MaxPool1DLayer(\n",
    "            self.l_conv1_1, \n",
    "            pool_size=pool_size_EEG\n",
    "        )\n",
    "        self.l_pool2 = lasagne.layers.MaxPool1DLayer(\n",
    "            self.l_conv2_1, \n",
    "            pool_size=pool_size_ACC\n",
    "        )    \n",
    "        #Two dense layers for each signal\n",
    "        self.l_dense_1_1 = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(\n",
    "                self.l_pool1, \n",
    "                p=drop_out\n",
    "            ),\n",
    "            num_units=num_units_EEG,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify\n",
    "        )\n",
    "        self.l_dense_2_1 = lasagne.layers.DenseLayer(\n",
    "                lasagne.layers.dropout(\n",
    "                self.l_pool2, \n",
    "                p=drop_out\n",
    "            ),\n",
    "            num_units=num_units_ACC,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify\n",
    "        )\n",
    "        #use neurons from both signals to predict\n",
    "        #concatenate neurons\n",
    "        self.l_concat = lasagne.layers.ConcatLayer(\n",
    "            [\n",
    "                self.l_dense_1_1, \n",
    "                self.l_dense_2_1\n",
    "            ], \n",
    "            axis=1\n",
    "        )\n",
    "        #dense layer with all neurons\n",
    "        self.l_dense_2 = lasagne.layers.DenseLayer(\n",
    "                lasagne.layers.dropout(\n",
    "                self.l_concat, \n",
    "                p=drop_out\n",
    "            ),\n",
    "            num_units=num_units_pool,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify\n",
    "        )\n",
    "        #Output layer\n",
    "        self.l_output = lasagne.layers.DenseLayer(\n",
    "            lasagne.layers.dropout(\n",
    "                self.l_dense_2, \n",
    "                p=drop_out\n",
    "            ),\n",
    "            num_units=num_classes,\n",
    "            nonlinearity=lasagne.nonlinearities.softmax\n",
    "        )\n",
    "        self.prediction = lasagne.layers.get_output(self.l_output)\n",
    "        self.loss = lasagne.objectives.multiclass_hinge_loss(self.prediction, target_var)\n",
    "        self.loss = self.loss.mean()\n",
    "        self.params = lasagne.layers.get_all_params(self.l_output, trainable=True)\n",
    "        self.updates = lasagne.updates.adadelta(\n",
    "            self.loss, \n",
    "            self.params, \n",
    "            learning_rate=init_learning_rate\n",
    "        )\n",
    "        self.test_prediction = lasagne.layers.get_output(self.l_output, deterministic=True)\n",
    "        self.test_loss = lasagne.objectives.multiclass_hinge_loss(self.test_prediction,\n",
    "                                                                target_var)\n",
    "        self.test_loss = self.test_loss.mean()\n",
    "        self.test_acc = T.mean(T.eq(T.argmax(self.test_prediction, axis=1), self.target_var),\n",
    "                  dtype=theano.config.floatX)\n",
    "        self.train_fn =theano.function(\n",
    "            [self.input_var_EEG, self.input_var_ACC, self.target_var], \n",
    "            self.loss, \n",
    "            updates=self.updates\n",
    "        )\n",
    "        self.val_fn = theano.function(\n",
    "            [self.input_var_EEG, \n",
    "             self.input_var_ACC,\n",
    "             self.target_var\n",
    "            ], \n",
    "            [self.test_loss, self.test_acc])\n",
    "    def fit(self,X_EEG_train,X_ACC_train,X_EEG_val,X_ACC_val,y_train,y_val):\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            # In each epoch, we do a full pass over the training data:\n",
    "            train_err = 0\n",
    "            train_batches = 0\n",
    "            start_time = time.time()\n",
    "            for batch in iterate_minibatches(X_EEG_train, X_ACC_train, y_train, 64, shuffle=True):\n",
    "                input_1, input_2, targets = batch\n",
    "                targets = targets.astype(np.int32)\n",
    "                train_err += self.train_fn(input_1, input_2, targets)\n",
    "                train_batches += 1\n",
    "                if train_batches % 100 == 0:\n",
    "                    print \"epoch {}, train batches{}\".format(epoch,train_batches) \n",
    "\n",
    "            # And a full pass over the validation data:\n",
    "            val_err = 0\n",
    "            val_acc = 0\n",
    "            val_batches = 0\n",
    "            for batch in iterate_minibatches(X_EEG_val, X_ACC_val, y_val, 128, shuffle=False):\n",
    "                input_1, input_2, targets = batch\n",
    "                targets = targets.astype(np.int32)\n",
    "                err, acc = self.val_fn(input_1, input_2, targets)\n",
    "                val_err += err\n",
    "                val_acc += acc\n",
    "                val_batches += 1\n",
    "\n",
    "            # Then we print the results for this epoch:\n",
    "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "                epoch + 1, num_epochs, time.time() - start_time))\n",
    "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "            print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "            print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
    "                val_acc / val_batches * 100))\n",
    "        return self\n",
    "        \n",
    "    def predict_proba(self,X_EEG_test, X_ACC_test):\n",
    "        net_output=lasagne.layers.get_output(self.l_output, deterministic=True)\n",
    "        prediction_fn = theano.function([input_var_EEG, input_var_ACC], [net_output])\n",
    "        prediction =  prediction_fn(X_EEG_test, X_ACC_test)\n",
    "        return prediction[0]\n",
    "\n",
    "    def predict(self,X_EEG_test, X_ACC_test):\n",
    "         return np.argmax(self.predict_proba(X_EEG_test, X_ACC_test), axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnn=CNN(input_var_EEG, \n",
    "        input_var_ACC, \n",
    "        target_var,\n",
    "        num_epochs\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train batches100\n",
      "epoch 0, train batches200\n",
      "epoch 0, train batches300\n",
      "Epoch 1 of 15 took 558.624s\n",
      "  training loss:\t\t0.979565\n",
      "  validation loss:\t\t0.926918\n",
      "  validation accuracy:\t\t57.66 %\n",
      "epoch 1, train batches100\n",
      "epoch 1, train batches200\n",
      "epoch 1, train batches300\n",
      "Epoch 2 of 15 took 557.725s\n",
      "  training loss:\t\t0.895965\n",
      "  validation loss:\t\t0.858384\n",
      "  validation accuracy:\t\t58.44 %\n",
      "epoch 2, train batches100\n",
      "epoch 2, train batches200\n",
      "epoch 2, train batches300\n",
      "Epoch 3 of 15 took 556.847s\n",
      "  training loss:\t\t0.848248\n",
      "  validation loss:\t\t0.835154\n",
      "  validation accuracy:\t\t58.26 %\n",
      "epoch 3, train batches100\n",
      "epoch 3, train batches200\n",
      "epoch 3, train batches300\n",
      "Epoch 4 of 15 took 556.360s\n",
      "  training loss:\t\t0.828741\n",
      "  validation loss:\t\t0.818017\n",
      "  validation accuracy:\t\t59.38 %\n",
      "epoch 4, train batches100\n",
      "epoch 4, train batches200\n",
      "epoch 4, train batches300\n",
      "Epoch 5 of 15 took 556.145s\n",
      "  training loss:\t\t0.810014\n",
      "  validation loss:\t\t0.811293\n",
      "  validation accuracy:\t\t59.60 %\n",
      "epoch 5, train batches100\n",
      "epoch 5, train batches200\n",
      "epoch 5, train batches300\n",
      "Epoch 6 of 15 took 555.562s\n",
      "  training loss:\t\t0.803782\n",
      "  validation loss:\t\t0.807824\n",
      "  validation accuracy:\t\t59.78 %\n",
      "epoch 6, train batches100\n",
      "epoch 6, train batches200\n",
      "epoch 6, train batches300\n",
      "Epoch 7 of 15 took 555.475s\n",
      "  training loss:\t\t0.791885\n",
      "  validation loss:\t\t0.810565\n",
      "  validation accuracy:\t\t59.34 %\n",
      "epoch 7, train batches100\n",
      "epoch 7, train batches200\n",
      "epoch 7, train batches300\n",
      "Epoch 8 of 15 took 558.356s\n",
      "  training loss:\t\t0.786353\n",
      "  validation loss:\t\t0.801477\n",
      "  validation accuracy:\t\t59.82 %\n",
      "epoch 8, train batches100\n",
      "epoch 8, train batches200\n",
      "epoch 8, train batches300\n",
      "Epoch 9 of 15 took 558.216s\n",
      "  training loss:\t\t0.779056\n",
      "  validation loss:\t\t0.789628\n",
      "  validation accuracy:\t\t60.71 %\n",
      "epoch 9, train batches100\n",
      "epoch 9, train batches200\n",
      "epoch 9, train batches300\n",
      "Epoch 10 of 15 took 557.423s\n",
      "  training loss:\t\t0.774426\n",
      "  validation loss:\t\t0.791970\n",
      "  validation accuracy:\t\t60.57 %\n",
      "epoch 10, train batches100\n",
      "epoch 10, train batches200\n",
      "epoch 10, train batches300\n",
      "Epoch 11 of 15 took 557.257s\n",
      "  training loss:\t\t0.766317\n",
      "  validation loss:\t\t0.801537\n",
      "  validation accuracy:\t\t60.01 %\n",
      "epoch 11, train batches100\n",
      "epoch 11, train batches200\n",
      "epoch 11, train batches300\n",
      "Epoch 12 of 15 took 558.921s\n",
      "  training loss:\t\t0.757310\n",
      "  validation loss:\t\t0.803241\n",
      "  validation accuracy:\t\t59.86 %\n",
      "epoch 12, train batches100\n",
      "epoch 12, train batches200\n",
      "epoch 12, train batches300\n",
      "Epoch 13 of 15 took 556.731s\n",
      "  training loss:\t\t0.744800\n",
      "  validation loss:\t\t0.770583\n",
      "  validation accuracy:\t\t61.57 %\n",
      "epoch 13, train batches100\n",
      "epoch 13, train batches200\n",
      "epoch 13, train batches300\n",
      "Epoch 14 of 15 took 556.192s\n",
      "  training loss:\t\t0.736956\n",
      "  validation loss:\t\t0.806970\n",
      "  validation accuracy:\t\t59.67 %\n",
      "epoch 14, train batches100\n",
      "epoch 14, train batches200\n",
      "epoch 14, train batches300\n",
      "Epoch 15 of 15 took 555.511s\n",
      "  training loss:\t\t0.730198\n",
      "  validation loss:\t\t0.762780\n",
      "  validation accuracy:\t\t62.43 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(input_var_ACC=inputs, input_var_EEG=inputs, num_epochs=15,\n",
       "  target_var=targets)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(X_EEG_train, X_ACC_train, X_EEG_val, X_ACC_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probas=cnn.predict_proba(X_EEG_test, X_ACC_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.77855810e-05,   4.82036697e-05,   9.99730767e-01,\n",
       "          3.43746934e-05,   1.18869070e-04],\n",
       "       [  9.91853397e-04,   8.18514844e-04,   9.96939142e-01,\n",
       "          1.78721561e-04,   1.07176800e-03],\n",
       "       [  2.36951934e-02,   1.71426373e-02,   1.06580616e-01,\n",
       "          8.15073205e-01,   3.75083477e-02],\n",
       "       ..., \n",
       "       [  1.10803278e-15,   1.46740571e-15,   3.94615369e-11,\n",
       "          1.00000000e+00,   2.77459189e-14],\n",
       "       [  3.86779706e-06,   1.89140304e-06,   2.06654852e-04,\n",
       "          9.99771590e-01,   1.59960886e-05],\n",
       "       [  1.06517770e-26,   2.03410796e-27,   3.97027722e-20,\n",
       "          1.00000000e+00,   1.66157973e-24]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions=cnn.predict(X_EEG_test, X_ACC_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Counter({2.0: 1537, 3.0: 945, 4.0: 438, 0.0: 151, 1.0: 42}) Counter({2: 1633, 3: 1480})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "print collections.Counter(y_test), collections.Counter(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearnearn.metrics import accuracy_score\n",
    "print accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=lasagne.layers.get_output(cnn.l_conv1_1, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=theano.function([input_var_EEG], [a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filt=f(X_EEG_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_scv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-397d8767bbb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfilt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_scv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/filters_EEG\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_scv'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m<ipython-input-27-397d8767bbb2>\u001b[0m(1)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m----> 1 \u001b[1;33m\u001b[0mfilt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_scv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/filters_EEG\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "filt.to_scv(\"data/filters_EEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_last_layer = theano.function([input_var_EEG, input_var_ACC],[lasagne.layers.get_output(cnn.l_dense_2, deterministic=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "BaseCorrMM: Failed to allocate output of 25214 x 32 x 1 x 3732\nApply node that caused the error: CorrMM{(0, 0), (1, 1)}(InplaceDimShuffle{0,1,x,2}.0, Subtensor{::, ::, ::int64, ::int64}.0)\nToposort index: 16\nInputs types: [TensorType(float64, (False, False, True, False)), TensorType(float64, (False, False, True, False))]\nInputs shapes: [(25214, 1, 1, 3751), (32, 1, 1, 20)]\nInputs strides: [(30008, 30008, 30008, 8), (160, 160, -160, -8)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Subtensor{::, ::, int64}(CorrMM{(0, 0), (1, 1)}.0, Constant{0})]]\n\nBacktrace when the node is created:\n  File \"lasagne/theano_extensions/conv.py\", line 71, in conv1d_mc0\n    filter_flip=filter_flip)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-b46fcf21663d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#last_layer_val=get_last_layer(X_EEG_val, X_ACC_val)[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlast_layer_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_last_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_EEG_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_ACC_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/data/home/sofia.calcagno/sleep/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    869\u001b[0m                     \u001b[0mnode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     storage_map=getattr(self.fn, 'storage_map', None))\n\u001b[0m\u001b[0;32m    872\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;31m# old-style linkers raise their own exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data/home/sofia.calcagno/sleep/theano/gof/link.pyc\u001b[0m in \u001b[0;36mraise_with_op\u001b[1;34m(node, thunk, exc_info, storage_map)\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;31m# extra long error message in that case.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m     \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/data/home/sofia.calcagno/sleep/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: BaseCorrMM: Failed to allocate output of 25214 x 32 x 1 x 3732\nApply node that caused the error: CorrMM{(0, 0), (1, 1)}(InplaceDimShuffle{0,1,x,2}.0, Subtensor{::, ::, ::int64, ::int64}.0)\nToposort index: 16\nInputs types: [TensorType(float64, (False, False, True, False)), TensorType(float64, (False, False, True, False))]\nInputs shapes: [(25214, 1, 1, 3751), (32, 1, 1, 20)]\nInputs strides: [(30008, 30008, 30008, 8), (160, 160, -160, -8)]\nInputs values: ['not shown', 'not shown']\nOutputs clients: [[Subtensor{::, ::, int64}(CorrMM{(0, 0), (1, 1)}.0, Constant{0})]]\n\nBacktrace when the node is created:\n  File \"lasagne/theano_extensions/conv.py\", line 71, in conv1d_mc0\n    filter_flip=filter_flip)\n\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1;32m/data/home/sofia.calcagno/sleep/theano/compile/function_module.py\u001b[0m(871)\u001b[0;36m__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m    870 \u001b[1;33m                    \u001b[0mthunk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthunk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m--> 871 \u001b[1;33m                    storage_map=getattr(self.fn, 'storage_map', None))\n",
      "\u001b[0m\u001b[1;32m    872 \u001b[1;33m            \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> u\n",
      "> \u001b[1;32m<ipython-input-41-b46fcf21663d>\u001b[0m(2)\u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m      1 \u001b[1;33m\u001b[1;31m#last_layer_val=get_last_layer(X_EEG_val, X_ACC_val)[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m----> 2 \u001b[1;33m\u001b[0mlast_layer_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_last_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_EEG_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_ACC_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> quit\n"
     ]
    }
   ],
   "source": [
    "#last_layer_val=get_last_layer(X_EEG_val, X_ACC_val)[0]\n",
    "last_layer_train=get_last_layer(X_EEG_train, X_ACC_train)[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et = ExtraTreesClassifier(random_state = 42,n_estimators=1000, max_depth= 29, n_jobs = 3, class_weight = 'balanced')\n",
    "et.fit(X_val, y_trai)\n",
    "predicted_label = et.predict(X_test)\n",
    "\n",
    "#print(\"Extra Random Trees - accuracy Score on test_data : \", accuracy_score(y_test, predicted_label))\n",
    "#print(\"Extra Random Trees - kappa Score on test_data : \", cohen_kappa_score(y_test, predicted_label))\n",
    "print(\"Extra Random Trees - kappa Score on train data : \", cohen_kappa_score(y_train, et.predict(X_train)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
